{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class SequenceTrimmer: not into the self.training at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceTrimmer(nn.Module):\n",
    "    '''\n",
    "    attributes:\n",
    "        enabled: bool -> to enable/disable the trimming\n",
    "        target: tuple (min, max) -> to set the target quantile range\n",
    "        _counter: int -> to count the number of forward calls\n",
    "    methods:\n",
    "        forward: to trim the sequence length\n",
    "    '''\n",
    "    def __init__(self, enabled=False, target=(0.9, 1.02), **kwargs) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "        self.enabled = enabled\n",
    "        self.target = target\n",
    "        self._counter = 0\n",
    "\n",
    "    def forward(self, x, v=None, mask=None, uu=None):\n",
    "        # x: (N, C, P)\n",
    "        # v: (N, 4, P) [px,py,pz,energy]\n",
    "        # mask: (N, 1, P) -- real particle = 1, padded = 0\n",
    "        # uu: (N, C', P, P)\n",
    "        if mask is None:\n",
    "            mask = torch.ones_like(x[:, :1])\n",
    "        mask = mask.bool()\n",
    "\n",
    "        if self.enabled:\n",
    "            if self._counter < 5:\n",
    "                self._counter += 1\n",
    "            else:\n",
    "                if self.training:\n",
    "                    \n",
    "                    q = torch.min(torch.ones(1, device=mask.device), torch.rand(1, device=mask.device) * (self.target[1] - self.target[0]) + self.target[0])[0]                    \n",
    "                    perm = rand.argsort(dim=-1, descending=True)\n",
    "                    maxlen = torch.quantile(mask.type_as(x).sum(dim=-1), q).long()\n",
    "                    rand = torch.rand_like(mask.type_as(x))\n",
    "                    rand.masked_fill_(~mask, -1)\n",
    "                    perm = rand.argsort(dim=-1, descending=True)  # (N, 1, P)\n",
    "                    mask = torch.gather(mask, -1, perm)\n",
    "                    x = torch.gather(x, -1, perm.expand_as(x))\n",
    "                    if v is not None:\n",
    "                        v = torch.gather(v, -1, perm.expand_as(v))\n",
    "                    if uu is not None:\n",
    "                        uu = torch.gather(uu, -2, perm.unsqueeze(-1).expand_as(uu))\n",
    "                        uu = torch.gather(uu, -1, perm.unsqueeze(-2).expand_as(uu))\n",
    "                else:\n",
    "                    maxlen = mask.sum(dim=-1).max()\n",
    "                maxlen = max(maxlen, 1)\n",
    "                if maxlen < mask.size(-1):\n",
    "                    mask = mask[:, :, :maxlen]\n",
    "                    x = x[:, :, :maxlen]\n",
    "                    if v is not None:\n",
    "                        v = v[:, :, :maxlen]\n",
    "                    if uu is not None:\n",
    "                        uu = uu[:, :, :maxlen, :maxlen]\n",
    "\n",
    "        return x, v, mask, uu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class Embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Embed(nn.Module):\n",
    "    def __init__(self, input_dim, dims, normalize_input=True, activation='gelu'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_bn = nn.BatchNorm1d(input_dim) if normalize_input else None\n",
    "        module_list = []\n",
    "        for dim in dims:\n",
    "            module_list.extend([\n",
    "                nn.LayerNorm(input_dim),\n",
    "                nn.Linear(input_dim, dim),\n",
    "                nn.GELU() if activation == 'gelu' else nn.ReLU(),\n",
    "            ])\n",
    "            input_dim = dim\n",
    "        self.embed = nn.Sequential(*module_list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.input_bn is not None:\n",
    "            # x: (batch, embed_dim, seq_len)\n",
    "            x = self.input_bn(x)\n",
    "            x = x.permute(2, 0, 1).contiguous()\n",
    "        # x: (seq_len, batch, embed_dim)\n",
    "        return self.embed(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_tril_indices():\n",
    "    \"\"\"\n",
    "    Returns the lower triangular indices of a square matrix.\n",
    "    # 中文解释\n",
    "    返回一个方阵的下三角索引。\n",
    "    Usage:\n",
    "        indices = torch.tril_indices(n, m, offset=0, dtype=torch.int64, device=None)\n",
    "\n",
    "    Parameters:\n",
    "        - n (int): The number of rows in the matrix.\n",
    "        - m (int): The number of columns in the matrix.\n",
    "        - offset (int, optional): The diagonal offset. Default is 0.\n",
    "        - dtype (torch.dtype, optional): The desired data type of the returned indices. Default is torch.int64.\n",
    "        - device (torch.device, optional): The desired device of the returned indices. Default is None.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor containing the lower triangular indices of the matrix.\n",
    "        # 中文解释：\n",
    "        返回一个包含矩阵下三角索引的张量。\n",
    "\n",
    "    Example:\n",
    "        import torch\n",
    "\n",
    "        n = 4\n",
    "        m = 4\n",
    "        indices = torch.tril_indices(n, m)\n",
    "        print(indices)\n",
    "\n",
    "    Output:\n",
    "        tensor([[0, 1, 2, 3],\n",
    "                [1, 2, 3, 3],\n",
    "                [2, 3, 3, 3],\n",
    "                [3, 3, 3, 3]])\n",
    "    \"\"\"\n",
    "    indices = torch.tril_indices(n, m, offset=0, dtype=torch.int64, device=None)\n",
    "    return indices\n",
    "torch.tril_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected ':' (2009823427.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[16], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    class Block\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expected ':'\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "class Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, embed_dim=128, num_heads=8, ffn_ratio=4,\n",
    "                 dropout=0.1, attn_dropout=0.1, activation_dropout=0.1,\n",
    "                 add_bias_kv=False, activation='gelu',\n",
    "                 scale_fc=True, scale_attn=True, scale_heads=True, scale_resids=True, enable_mem_efficient=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.ffn_dim = embed_dim * ffn_ratio\n",
    "\n",
    "        self.pre_attn_norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.enable_mem_efficient = enable_mem_efficient\n",
    "        if enable_mem_efficient:\n",
    "            from weaver.utils.import_tools import import_module\n",
    "            import os\n",
    "            MultiheadAttention = import_module(os.path.join(os.path.dirname(__file__), 'memory_efficient_attention.py'), 'memory_efficient_attention').MemoryEfficientMultiheadAttention\n",
    "        else:\n",
    "            MultiheadAttention = nn.MultiheadAttention\n",
    "        self.attn = MultiheadAttention(\n",
    "            embed_dim,\n",
    "            num_heads,\n",
    "            dropout=attn_dropout,\n",
    "            add_bias_kv=add_bias_kv,\n",
    "        )\n",
    "        self.post_attn_norm = nn.LayerNorm(embed_dim) if scale_attn else None\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.pre_fc_norm = nn.LayerNorm(embed_dim)\n",
    "        self.fc1 = nn.Linear(embed_dim, self.ffn_dim)\n",
    "        self.act = nn.GELU() if activation == 'gelu' else nn.ReLU()\n",
    "        self.act_dropout = nn.Dropout(activation_dropout)\n",
    "        self.post_fc_norm = nn.LayerNorm(self.ffn_dim) if scale_fc else None\n",
    "        self.fc2 = nn.Linear(self.ffn_dim, embed_dim)\n",
    "\n",
    "        self.c_attn = nn.Parameter(torch.ones(num_heads), requires_grad=True) if scale_heads else None\n",
    "        self.w_resid = nn.Parameter(torch.ones(embed_dim), requires_grad=True) if scale_resids else None\n",
    "\n",
    "    def forward(self, x, x_cls=None, padding_mask=None, attn_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`\n",
    "            x_cls (Tensor, optional): class token input to the layer of shape `(1, batch, embed_dim)`\n",
    "            padding_mask (ByteTensor, optional): binary\n",
    "                ByteTensor of shape `(batch, seq_len)` where padding\n",
    "                elements are indicated by ``1``.\n",
    "        Returns:\n",
    "            encoded output of shape `(seq_len, batch, embed_dim)`\n",
    "        \"\"\"\n",
    "\n",
    "        if not self.enable_mem_efficient:\n",
    "            padding_mask = torch.zeros_like(padding_mask, dtype=x.dtype).masked_fill(padding_mask, float('-inf'))\n",
    "        if x_cls is not None:\n",
    "            with torch.no_grad():\n",
    "                # prepend one element for x_cls: -> (batch, 1+seq_len)\n",
    "                padding_mask = torch.cat((torch.zeros_like(padding_mask[:, :1]), padding_mask), dim=1)\n",
    "            # class attention: https://arxiv.org/pdf/2103.17239.pdf\n",
    "            residual = x_cls\n",
    "            u = torch.cat((x_cls, x), dim=0)  # (seq_len+1, batch, embed_dim)\n",
    "            u = self.pre_attn_norm(u)\n",
    "            x = self.attn(x_cls, u, u, key_padding_mask=padding_mask, need_weights=False)[0]  # (1, batch, embed_dim)\n",
    "        else:\n",
    "            residual = x\n",
    "            x = self.pre_attn_norm(x)\n",
    "            x = self.attn(x, x, x, key_padding_mask=padding_mask,\n",
    "                          attn_mask=attn_mask, need_weights=False)[0]  # (seq_len, batch, embed_dim)\n",
    "\n",
    "        if self.c_attn is not None:\n",
    "            tgt_len = x.size(0)\n",
    "            x = x.view(tgt_len, -1, self.num_heads, self.head_dim)\n",
    "            x = torch.einsum('tbhd,h->tbdh', x, self.c_attn)\n",
    "            x = x.reshape(tgt_len, -1, self.embed_dim)\n",
    "        if self.post_attn_norm is not None:\n",
    "            x = self.post_attn_norm(x)\n",
    "        x = self.dropout(x)\n",
    "        x += residual\n",
    "\n",
    "        residual = x\n",
    "        x = self.pre_fc_norm(x)\n",
    "        x = self.act(self.fc1(x))\n",
    "        x = self.act_dropout(x)\n",
    "        if self.post_fc_norm is not None:\n",
    "            x = self.post_fc_norm(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        if self.w_resid is not None:\n",
    "            residual = torch.mul(self.w_resid, residual)\n",
    "        x += residual\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q: class Block 的 残差处理，为什么需要torch.mul (89行)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
